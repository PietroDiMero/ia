# Configuration générale
provider: "dummy"   # "dummy" | "openai" | "custom"
model: "gpt-4o-mini"  # ignoré si provider=dummy

evaluation:
  daily_sample_size: 50
  min_gain: 0.02          # +2% mini pour promouvoir
  judge_llm: false        # true pour LLM-as-judge si dispo
  fail_keywords: ["danger", "illegal", "destructive"]
  parallel_workers: auto  # auto = os.cpu_count()-1, ou un entier (ex: 8)

paths:
  tests_file: "data/tests.jsonl"
  rubric_file: "tests/rubric.yaml"
  active_prompt: "prompts/active_prompt.txt"
  candidates:
    - "prompts/system_pedagogue.txt"
    - "prompts/system_senior.txt"
  logs_dir: "logs"

scheduler:
  enabled: true
  interval_minutes: 5   # 1 pour test intensif, 5 conseillé en continu
  interval_seconds: 5   # utilisé si burst=true
  burst: false          # si true, enchaîne les cycles avec un petit délai (interval_seconds)
  min_promotion_gain: 0.01   # gain minimal vs actif pour promouvoir
  cooldown_minutes: 30       # anti-promotions en rafale
  sample_tests: 50           # nb max de tests tirés au sort par cycle
  script_timeout_seconds: 180  # coupe une exécution pour éviter les blocages UI

self_update:
  enabled: false              # activer pour tenter des changements de code automatiques
  min_gain: 0.01              # amélioration minimale de avg_score pour accepter le patch
  allow_paths: ["app/", "scripts/", "prompts/"]  # répertoires autorisés
  max_files: 3                # limite de fichiers modifiés par itération
  dry_run: false              # true pour proposer sans appliquer
  explain: true               # garder une note d'explication dans logs
